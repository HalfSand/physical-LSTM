{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2fc1334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import xml.etree.ElementTree\n",
    "from collections import defaultdict, namedtuple\n",
    "import itertools\n",
    "import numpy as np\n",
    "import scipy.interpolate\n",
    "import random\n",
    "import pysparkling\n",
    "import scipy.io\n",
    "import shutil\n",
    "import argparse\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74892329",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrackRow = namedtuple('Row', ['frame', 'car_id', 'x', 'y', 'xVelocity', 'yVelocity','prediction_number', 'scene_id'])\n",
    "TrackRow.__new__.__defaults__ = (None, None, None, None, None, None, None, None)\n",
    "SceneRow = namedtuple('Row', ['scene', 'car_id', 'start', 'end', 'fps', 'tag'])\n",
    "SceneRow.__new__.__defaults__ = (None, None, None, None, None, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbf3abbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Car:\n",
    "    def __init__(self):\n",
    "        self.scale_factors = None\n",
    "        self.temp = None\n",
    "\n",
    "    def calculate_scale_factors(self, df):\n",
    "        # 提取你需要的列\n",
    "        columns = ['x', 'y', 'xVelocity', 'yVelocity']\n",
    "\n",
    "        # 找到每列的最大值并除以100\n",
    "        self.scale_factors = [df[col].abs().max() / 100 for col in columns]\n",
    "\n",
    "    def read_line(self, line):\n",
    "        line = [e for e in line.split('\\t') if e != '']\n",
    "        if len(line) != 6:\n",
    "            print(f\"Unexpected line format: {line}\")\n",
    "            return None  # 或者处理错误的方式\n",
    "        return TrackRow(\n",
    "            int(float(line[0])),\n",
    "            int(float(line[1])),\n",
    "            float(line[2]),\n",
    "            float(line[3]),\n",
    "            float(line[4]),\n",
    "            float(line[5])\n",
    "        )\n",
    "\n",
    "    def read_csv(self, sc, input_file):\n",
    "        print('processing ' + input_file)\n",
    "\n",
    "        # 使用 pandas 读取 csv 文件\n",
    "        df = pd.read_csv(input_file)\n",
    "\n",
    "        # 选取你需要的列\n",
    "        df = df[['frame', 'id', 'x', 'y', 'xVelocity', 'yVelocity']]\n",
    "\n",
    "        # 计算缩放因子\n",
    "        self.calculate_scale_factors(df)\n",
    "\n",
    "        # 应用缩放因子\n",
    "        for i, col in enumerate(['x', 'y', 'xVelocity', 'yVelocity']):\n",
    "            df[col] = df[col] / self.scale_factors[i]\n",
    "\n",
    "        # 获取输入文件的路径和文件名，用于生成输出文件的路径\n",
    "        dir_path, file_name = os.path.split(input_file)\n",
    "        base_name, _ = os.path.splitext(file_name)\n",
    "        self.temp = os.path.join(dir_path, base_name + '.txt')\n",
    "\n",
    "        # 将 DataFrame 对象写入 txt 文件，不包含列名和索引，每列的值之间用 '\\t' 分隔\n",
    "        df.to_csv(self.temp, sep='\\t', header=False, index=False)\n",
    "\n",
    "        return (sc\n",
    "                .textFile(self.temp)\n",
    "                .map(self.read_line)\n",
    "                .cache())\n",
    "\n",
    "    def delete_temp(self):\n",
    "        if self.temp is not None and os.path.exists(self.temp):\n",
    "            os.remove(self.temp)\n",
    "            print(f\"Temp file {self.temp} has been deleted.\")\n",
    "        else:\n",
    "            print(\"No temp file to delete.\")\n",
    "\n",
    "    def append_scaler(self, ndjson_file):\n",
    "        if self.scale_factors is not None:\n",
    "            scale_dict = {'scaler':\n",
    "                              {'x': self.scale_factors[0],\n",
    "                               'y': self.scale_factors[1],\n",
    "                               'xVelocity': self.scale_factors[2],\n",
    "                               'yVelocity': self.scale_factors[3]}}\n",
    "            with open(ndjson_file, 'a') as f:\n",
    "                f.write(json.dumps(scale_dict) + '\\n')\n",
    "        else:\n",
    "            print(\"scale_factors is None.\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "230e687b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reader(object):\n",
    "    \"\"\"Read trajnet files.\n",
    "\n",
    "    :param scene_type: None -> numpy.array, 'rows' -> TrackRow and SceneRow, 'paths': grouped rows (primary pedestrian first), 'tags': numpy.array and scene tag\n",
    "    :param image_file: Associated image file of the scene\n",
    "    \"\"\"\n",
    "    def __init__(self, input_file, scene_type=None, image_file=None):\n",
    "        if scene_type is not None and scene_type not in {'rows', 'paths', 'tags'}:\n",
    "            raise Exception('scene_type not supported')\n",
    "        self.scene_type = scene_type\n",
    "\n",
    "        self.tracks_by_frame = defaultdict(list)\n",
    "        self.scenes_by_id = dict()\n",
    "\n",
    "        self.read_file(input_file)\n",
    "\n",
    "    def read_file(self, input_file):\n",
    "        with open(input_file, 'r') as f:\n",
    "            for line in f:\n",
    "                line = json.loads(line)\n",
    "                track = line.get('track')\n",
    "                if track is not None:\n",
    "                    row = TrackRow(track['f'], track['c'], track['x'], track['y'], track['xVelocity'], track['yVelocity'],\n",
    "                                   track.get('prediction_number'), track.get('scene_id'))\n",
    "                    self.tracks_by_frame[row.frame].append(row)\n",
    "                    continue\n",
    "                scene = line.get('scene')\n",
    "                if scene is not None:\n",
    "                    row = SceneRow(scene['id'], scene['c'], scene['s'], scene['e'],\n",
    "                                   scene.get('fps'), scene.get('tag'))\n",
    "                    self.scenes_by_id[row.scene] = row\n",
    "\n",
    "    def scenes(self, randomize=False, limit=0, ids=None, sample=None):\n",
    "        scene_ids = self.scenes_by_id.keys()\n",
    "        if ids is not None:\n",
    "            scene_ids = ids\n",
    "        if randomize:\n",
    "            scene_ids = list(scene_ids)\n",
    "            random.shuffle(scene_ids)\n",
    "        if limit:\n",
    "            scene_ids = itertools.islice(scene_ids, limit)\n",
    "        if sample is not None:\n",
    "            scene_ids = random.sample(scene_ids, int(len(scene_ids) * sample))\n",
    "        for scene_id in scene_ids:\n",
    "            yield self.scene(scene_id)\n",
    "\n",
    "    @staticmethod\n",
    "    def track_rows_to_paths(primary_car, track_rows):\n",
    "        primary_path = []\n",
    "        other_paths = defaultdict(list)\n",
    "        for row in track_rows:\n",
    "            if row.car_id == primary_car:\n",
    "                primary_path.append(row)\n",
    "                continue\n",
    "            other_paths[row.car_id].append(row)\n",
    "\n",
    "        return [primary_path] + list(other_paths.values())\n",
    "\n",
    "    @staticmethod\n",
    "    def paths_to_xyv(paths):\n",
    "        \"\"\"Convert paths to numpy array with nan as blanks.\"\"\"\n",
    "        frames = set(r.frame for r in paths[0])\n",
    "        cars = set(row.car_id\n",
    "                          for path in paths\n",
    "                          for row in path if row.frame in frames)\n",
    "        paths = [path for path in paths if path[0].car_id in cars]\n",
    "        frames = sorted(frames)\n",
    "        cars = list(cars)\n",
    "\n",
    "        frame_to_index = {frame: i for i, frame in enumerate(frames)}\n",
    "        xyv = np.full((len(frames), len(cars), 4), np.nan)\n",
    "\n",
    "        for car_index, path in enumerate(paths):\n",
    "            for row in path:\n",
    "                if row.frame not in frame_to_index:\n",
    "                    continue\n",
    "                entry = xyv[frame_to_index[row.frame]][car_index]\n",
    "                entry[0] = row.x\n",
    "                entry[1] = row.y\n",
    "                entry[2] = row.xVelocity\n",
    "                entry[3] = row.yVelocity\n",
    "\n",
    "        return xyv\n",
    "\n",
    "    def scene(self, scene_id):\n",
    "        scene = self.scenes_by_id.get(scene_id)\n",
    "        if scene is None:\n",
    "            raise Exception('scene with that id not found')\n",
    "\n",
    "        frames = range(scene.start, scene.end + 1)\n",
    "        track_rows = [r\n",
    "                      for frame in frames\n",
    "                      for r in self.tracks_by_frame.get(frame, [])]\n",
    "\n",
    "        # return as rows\n",
    "        if self.scene_type == 'rows':\n",
    "            return scene_id, scene.id, track_rows\n",
    "\n",
    "        # return as paths\n",
    "        paths = self.track_rows_to_paths(scene.car_id, track_rows)\n",
    "        if self.scene_type == 'paths':\n",
    "            return scene_id, paths\n",
    "\n",
    "        # return with scene tag\n",
    "        if self.scene_type == 'tags':\n",
    "            return scene_id, scene.tag, self.paths_to_xyv(paths)\n",
    "\n",
    "        # return a numpy array\n",
    "        return scene_id, self.paths_to_xyv(paths)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4be93805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trajnet_tracks(row):\n",
    "    #     x = round(row.x, 2)\n",
    "    #     y = round(row.y, 2)\n",
    "    #     if row.xVelocity is not None:\n",
    "    #         xVelocity = round(row.xVelocity, 2)\n",
    "    #     else:\n",
    "    #         xVelocity = None\n",
    "    #     if row.yVelocity is not None:\n",
    "    #         yVelocity = round(row.yVelocity, 2)\n",
    "    #     else:\n",
    "    #         yVelocity = None\n",
    "\n",
    "    x = row.x\n",
    "    y = row.y\n",
    "    xVelocity = row.xVelocity\n",
    "    yVelocity = row.yVelocity\n",
    "\n",
    "    if row.prediction_number is None:\n",
    "        return json.dumps({'track': {'f': row.frame, 'c': row.car_id, 'x': x, 'y': y,\n",
    "                                     'xVelocity': xVelocity, 'yVelocity': yVelocity}})\n",
    "    return json.dumps({'track': {'f': row.frame, 'c': row.car_id, 'x': x, 'y': y,\n",
    "                                 'xVelocity': xVelocity,\n",
    "                                 'yVelocity': yVelocity,\n",
    "                                 'prediction_number': row.prediction_number,\n",
    "                                 'scene_id': row.scene_id}})\n",
    "\n",
    "\n",
    "def trajnet_scenes(row):\n",
    "    return json.dumps({'scene': {'id': row.scene, 'c': row.car_id, 's': row.start, 'e': row.end,\n",
    "                                 'fps': row.fps, 'tag': row.tag}})\n",
    "\n",
    "\n",
    "def trajnet(row):\n",
    "    if isinstance(row, TrackRow):\n",
    "        return trajnet_tracks(row)\n",
    "    if isinstance(row, SceneRow):\n",
    "        return trajnet_scenes(row)\n",
    "\n",
    "    raise Exception('unknown row type')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3e1f66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type(scene, args):\n",
    "    '''\n",
    "    Categorization of Single Scene\n",
    "    :param scene: All trajectories as TrackRows, args\n",
    "    :return: The type of the traj\n",
    "    '''\n",
    "\n",
    "    ## Get xy-coordinates from trackRows\n",
    "    scene_xy = Reader.paths_to_xyv(scene)\n",
    "\n",
    "    # Type 1\n",
    "    def euclidean_distance(row1, row2):\n",
    "        \"\"\"Euclidean distance squared between two rows.\"\"\"\n",
    "        return np.sqrt((row1.x - row2.x) ** 2 + (row1.y - row2.y) ** 2)\n",
    "\n",
    "    #     ## Type 2\n",
    "    #     def linear_system(scene, obs_len, pred_len):\n",
    "    #         '''\n",
    "    #         return: True if the traj is linear according to Kalman\n",
    "    #         '''\n",
    "    #         kalman_prediction, _ = kalman_predict(scene, obs_len, pred_len)[0]\n",
    "    #         return trajnetplusplustools.metrics.final_l2(scene[0], kalman_prediction)\n",
    "\n",
    "    #     ## Type 3\n",
    "    #     def interaction(rows, pos_range, dist_thresh, obs_len):\n",
    "    #         '''\n",
    "    #         :return: Determine if interaction exists and type (optionally)\n",
    "    #         '''\n",
    "    #         interaction_matrix = check_interaction(rows, pos_range=pos_range, \\\n",
    "    #                                  dist_thresh=dist_thresh, obs_len=obs_len)\n",
    "    #         return np.any(interaction_matrix)\n",
    "\n",
    "    # Category Tags\n",
    "    mult_tag = []\n",
    "    sub_tag = []\n",
    "\n",
    "    # Static\n",
    "    if euclidean_distance(scene[0][0], scene[0][-1]) < args.static_threshold:\n",
    "        mult_tag.append(1)\n",
    "\n",
    "    #     # Linear\n",
    "    #     elif linear_system(scene, args.obs_len, args.pred_len) < args.linear_threshold:\n",
    "    #         mult_tag.append(2)\n",
    "\n",
    "    #     # Interactions\n",
    "    #     elif interaction(scene_xy, args.inter_pos_range, args.inter_dist_thresh, args.obs_len) \\\n",
    "    #          or np.any(group(scene_xy, args.grp_dist_thresh, args.grp_std_thresh, args.obs_len)):\n",
    "    #         mult_tag.append(3)\n",
    "\n",
    "    # Non-Linear (No explainable reason)\n",
    "    else:\n",
    "        mult_tag.append(4)\n",
    "\n",
    "    #     # Interaction Types\n",
    "    #     if mult_tag[0] == 3:\n",
    "    #         sub_tag = get_interaction_type(scene_xy, args.inter_pos_range,\n",
    "    #                                        args.inter_dist_thresh, args.obs_len)\n",
    "    #     else:\n",
    "    #         sub_tag = []\n",
    "    sub_tag = []\n",
    "    return mult_tag[0], mult_tag, sub_tag\n",
    "\n",
    "\n",
    "def write_sf(rows, path, new_scenes, new_frames):\n",
    "    \"\"\" Writing scenes with categories \"\"\"\n",
    "    output_path = path.replace('output_pre', 'output')\n",
    "    pysp_tracks = rows.filter(lambda r: r.frame in new_frames).map(trajnet)\n",
    "    pysp_scenes = pysparkling.Context().parallelize(new_scenes).map(trajnet)\n",
    "    pysp_scenes.union(pysp_tracks).saveAsTextFile(output_path)\n",
    "\n",
    "\n",
    "def trajectory_type(rows, path, fps, track_id=0, args=None):\n",
    "    \"\"\" Categorization of all scenes \"\"\"\n",
    "\n",
    "    # Read\n",
    "    reader = Reader(path, scene_type='paths')\n",
    "    scenes = [s for _, s in reader.scenes()]\n",
    "    # Filtered Frames and Scenes\n",
    "    new_frames = set()\n",
    "    new_scenes = []\n",
    "\n",
    "    start_frames = set()\n",
    "    ###########################################################################\n",
    "    # scenes_test helps to handle both test and test_private simultaneously\n",
    "    # scenes_test correspond to Test\n",
    "    ###########################################################################\n",
    "    test = 'test' in path\n",
    "    if test:\n",
    "        path_test = path.replace('test_private', 'test')\n",
    "        reader_test = Reader(path_test, scene_type='paths')\n",
    "        scenes_test = [s for _, s in reader_test.scenes()]\n",
    "        # Filtered Test Frames and Test Scenes\n",
    "        new_frames_test = set()\n",
    "        new_scenes_test = []\n",
    "\n",
    "    # Initialize Tag Stats to be collected\n",
    "    tags = {1: [], 2: [], 3: [], 4: []}\n",
    "    mult_tags = {1: [], 2: [], 3: [], 4: []}\n",
    "    sub_tags = {1: [], 2: [], 3: [], 4: []}\n",
    "    col_count = 0\n",
    "\n",
    "    if not scenes:\n",
    "        raise Exception('No scenes found')\n",
    "\n",
    "    for index, scene in enumerate(scenes):\n",
    "        if (index + 1) % 50 == 0:\n",
    "            print(index)\n",
    "\n",
    "        # Primary Path\n",
    "        car_interest = scene[0]\n",
    "\n",
    "        # Assert Test Scene length\n",
    "        if test:\n",
    "            assert len(scenes_test[index][0]) >= args.obs_len, \\\n",
    "                'Scene Test not adequate length'\n",
    "\n",
    "        # Get Tag\n",
    "        tag, mult_tag, sub_tag = get_type(scene, args)\n",
    "\n",
    "        if np.random.uniform() < args.acceptance[tag - 1]:\n",
    "            # Check Validity\n",
    "\n",
    "            # Update Tags\n",
    "            tags[tag].append(track_id)\n",
    "            for tt in mult_tag:\n",
    "                mult_tags[tt].append(track_id)\n",
    "            for st in sub_tag:\n",
    "                sub_tags[st].append(track_id)\n",
    "\n",
    "            # Define Scene_Tag\n",
    "            scene_tag = []\n",
    "            scene_tag.append(tag)\n",
    "            scene_tag.append(sub_tag)\n",
    "\n",
    "            new_frames |= set(car_interest[i].frame for i in range(len(car_interest)))\n",
    "            new_scenes.append(\n",
    "                SceneRow(track_id, car_interest[0].car_id,\n",
    "                         car_interest[0].frame, car_interest[-1].frame,\n",
    "                         fps, scene_tag))\n",
    "\n",
    "            # Append to list of scenes_test as well if Test Set\n",
    "            if test:\n",
    "                new_frames_test |= set(car_interest[i].frame for i in range(args.obs_len))\n",
    "                new_scenes_test.append(\n",
    "                    SceneRow(track_id, car_interest[0].car_id,\n",
    "                             car_interest[0].frame, car_interest[-1].frame,\n",
    "                             fps, 0))\n",
    "\n",
    "            track_id += 1\n",
    "\n",
    "    # Writes the Final Scenes and Frames\n",
    "    write_sf(rows, path, new_scenes, new_frames)\n",
    "    if test:\n",
    "        write_sf(rows, path_test, new_scenes_test, new_frames_test)\n",
    "\n",
    "    # Stats\n",
    "\n",
    "    # Number of collisions found\n",
    "    print(\"Col Count: \", col_count)\n",
    "\n",
    "    if scenes:\n",
    "        print(\"Total Scenes: \", index)\n",
    "\n",
    "        # Types:\n",
    "        print(\"Main Tags\")\n",
    "        print(\"Type 1: \", len(tags[1]), \"Type 2: \", len(tags[2]),\n",
    "              \"Type 3: \", len(tags[3]), \"Type 4: \", len(tags[4]))\n",
    "        print(\"Sub Tags\")\n",
    "        print(\"LF: \", len(sub_tags[1]), \"CA: \", len(sub_tags[2]),\n",
    "              \"Group: \", len(sub_tags[3]), \"Others: \", len(sub_tags[4]))\n",
    "\n",
    "    return track_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "596264f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scenes(object):\n",
    "    def __init__(self, fps, start_scene_id=0, args=None):\n",
    "        self.scene_id = start_scene_id\n",
    "        self.chunk_size = args.obs_len + args.pred_len\n",
    "        self.chunk_stride = args.chunk_stride\n",
    "        self.obs_len = args.obs_len\n",
    "        self.visible_chunk = None\n",
    "        self.frames = set()\n",
    "        self.fps = fps\n",
    "        self.min_length = args.min_length\n",
    "\n",
    "    @staticmethod\n",
    "    def euclidean_distance_2(row1, row2):\n",
    "        \"\"\"Euclidean distance squared between two rows.\"\"\"\n",
    "        return (row1.x - row2.x)**2 + (row1.y - row2.y)**2\n",
    "\n",
    "    @staticmethod\n",
    "    def close_cars(rows, cell_size=10):\n",
    "        \"\"\"Fast computation of spatially close pedestrians.\n",
    "        By frame, get the list of pedestrian ids that or close to other\n",
    "        pedestrians. Approximate with multi-occupancy of discrete grid cells.\n",
    "        \"\"\"\n",
    "        sparse_occupancy = defaultdict(list)\n",
    "        for row in rows:\n",
    "            x = int(row.x // cell_size * cell_size)\n",
    "            y = int(row.y // cell_size * cell_size)\n",
    "            sparse_occupancy[(x, y)].append(row.car_id)\n",
    "        return {car_id\n",
    "                for cell in sparse_occupancy.values() if len(cell) > 1\n",
    "                for car_id in cell}\n",
    "\n",
    "    @staticmethod\n",
    "    def continuous_frames(frames, tolerance=1.5):\n",
    "        increments = [f2 - f1 for f1, f2 in zip(frames[:-1], frames[1:])]\n",
    "        median_increment = sorted(increments)[int(len(increments) / 2)]\n",
    "        ok = median_increment * tolerance > max(increments)\n",
    "\n",
    "        if not ok:\n",
    "            print('!!!!!!!!! DETECTED GAP IN FRAMES')\n",
    "            print(increments)\n",
    "\n",
    "        return ok\n",
    "\n",
    "    def from_rows(self, rows):\n",
    "        count_by_frame = rows.groupBy(lambda r: r.frame).mapValues(len).collectAsMap()\n",
    "        occupancy_by_frame = (rows\n",
    "                              .groupBy(lambda r: r.frame)\n",
    "                              .mapValues(self.close_cars)\n",
    "                              .collectAsMap())\n",
    "\n",
    "        def to_scene_row(car_frames):\n",
    "            car_id, scene_frames = car_frames\n",
    "            row = SceneRow(self.scene_id, car_id, scene_frames[0], scene_frames[-1], self.fps, 0)\n",
    "            self.scene_id += 1\n",
    "            return row\n",
    "\n",
    "        # scenes: pedestrian of interest, [frames]\n",
    "        scenes = (\n",
    "            rows\n",
    "            .groupBy(lambda r: r.car_id)\n",
    "            .filter(lambda car_path: len(car_path[1]) >= self.chunk_size)\n",
    "            .mapValues(lambda path: sorted(path, key=lambda car: car.frame))\n",
    "            .flatMapValues(lambda path: [\n",
    "                [path[ii].frame for ii in range(i, i + self.chunk_size)]\n",
    "                for i in range(0, len(path) - self.chunk_size + 1, self.chunk_stride)\n",
    "                # filter for pedestrians moving by more than min_length meter\n",
    "                if self.euclidean_distance_2(path[i], path[i+self.chunk_size-1]) > self.min_length\n",
    "            ])\n",
    "\n",
    "            # filter out scenes with large gaps in frame numbers\n",
    "            .filter(lambda car_frames: self.continuous_frames(car_frames[1]))\n",
    "\n",
    "            # filter for scenes that have some activity\n",
    "            .filter(lambda car_frames:\n",
    "                    sum(count_by_frame[f] for f in car_frames[1]) >= 2.0 * self.chunk_size)\n",
    "\n",
    "            # require some proximity to other pedestrians\n",
    "            .filter(lambda car_frames:\n",
    "                    car_frames[0] in {p\n",
    "                                      for frame in car_frames[1]\n",
    "                                      for p in occupancy_by_frame[frame]})\n",
    "\n",
    "            .cache()\n",
    "        )\n",
    "\n",
    "        self.frames |= set(scenes\n",
    "                           .flatMap(lambda car_frames:\n",
    "                                    car_frames[1]\n",
    "                                    if self.visible_chunk is None\n",
    "                                    else car_frames[1][:self.visible_chunk])\n",
    "                           .toLocalIterator())\n",
    "\n",
    "        return scenes.map(to_scene_row)\n",
    "\n",
    "    def rows_to_file(self, rows, output_file):\n",
    "        if '/test/' in output_file:\n",
    "            self.visible_chunk = self.obs_len\n",
    "        else:\n",
    "            self.visible_chunk = None\n",
    "        scenes = self.from_rows(rows)\n",
    "        tracks = rows.filter(lambda r: r.frame in self.frames)\n",
    "        all_data = rows.context.union((scenes, tracks))\n",
    "\n",
    "        # removes the file, if previously generated\n",
    "        if os.path.isfile(output_file):\n",
    "            os.remove(output_file)\n",
    "\n",
    "        # write scenes and tracks\n",
    "        all_data.map(trajnet).saveAsTextFile(output_file)\n",
    "\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cc63894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(line):\n",
    "    line = json.loads(line)\n",
    "    track = line.get('track')\n",
    "    if track is not None:\n",
    "        return TrackRow(track['f'], track['c'], track['x'], track['y'], track['xVelocity'], track['yVelocity'],\n",
    "                        track.get('prediction_number'))\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_trackrows(sc, input_file):\n",
    "    print('processing ' + input_file)\n",
    "    return (sc\n",
    "            .textFile(input_file)\n",
    "            .map(read_json)\n",
    "            .filter(lambda r: r is not None)\n",
    "            .cache())\n",
    "\n",
    "\n",
    "def write(input_rows, output_file, args):\n",
    "    \"\"\" Write Valid Scenes without categorization \"\"\"\n",
    "\n",
    "    print(\" Entering Writing \")\n",
    "    # To handle two different time stamps 7:00 and 17:00 of cff\n",
    "    if args.order_frames:\n",
    "        frames = sorted(set(input_rows.map(lambda r: r.frame).toLocalIterator()),\n",
    "                        key=lambda frame: frame % 100000)\n",
    "    else:\n",
    "        frames = sorted(set(input_rows.map(lambda r: r.frame).toLocalIterator()))\n",
    "\n",
    "    # split\n",
    "    train_split_index = int(len(frames) * args.train_fraction)\n",
    "    val_split_index = train_split_index + int(len(frames) * args.val_fraction)\n",
    "    train_frames = set(frames[:train_split_index])\n",
    "    val_frames = set(frames[train_split_index:val_split_index])\n",
    "    test_frames = set(frames[val_split_index:])\n",
    "\n",
    "    # train dataset\n",
    "    train_rows = input_rows.filter(lambda r: r.frame in train_frames)\n",
    "    train_output = output_file.format(split='train')\n",
    "    train_scenes = Scenes(fps=args.fps, start_scene_id=0, args=args).rows_to_file(train_rows, train_output)\n",
    "\n",
    "    # validation dataset\n",
    "    val_rows = input_rows.filter(lambda r: r.frame in val_frames)\n",
    "    val_output = output_file.format(split='val')\n",
    "    val_scenes = Scenes(fps=args.fps, start_scene_id=train_scenes.scene_id, args=args).rows_to_file(val_rows, val_output)\n",
    "\n",
    "    # public test dataset\n",
    "    test_rows = input_rows.filter(lambda r: r.frame in test_frames)\n",
    "    test_output = output_file.format(split='test')\n",
    "    test_scenes = Scenes(fps=args.fps, start_scene_id=val_scenes.scene_id, args=args)  # !!! Chunk Stride\n",
    "    test_scenes.rows_to_file(test_rows, test_output)\n",
    "\n",
    "    # private test dataset\n",
    "    private_test_output = output_file.format(split='test_private')\n",
    "    private_test_scenes = Scenes(fps=args.fps, start_scene_id=val_scenes.scene_id, args=args)\n",
    "    private_test_scenes.rows_to_file(test_rows, private_test_output)\n",
    "\n",
    "\n",
    "def categorize(sc, input_file, args):\n",
    "    \"\"\" Categorize the Scenes \"\"\"\n",
    "\n",
    "    print(\" Entering Categorizing \")\n",
    "    test_fraction = 1 - args.train_fraction - args.val_fraction\n",
    "\n",
    "    train_id = 0\n",
    "    if args.train_fraction:\n",
    "        print(\"Categorizing Training Set\")\n",
    "        train_rows = get_trackrows(sc, input_file.replace('split', '').format('train'))\n",
    "        train_id = trajectory_type(train_rows, input_file.replace('split', '').format('train'),\n",
    "                                   fps=args.fps, track_id=0, args=args)\n",
    "\n",
    "    val_id = train_id\n",
    "    if args.val_fraction:\n",
    "        print(\"Categorizing Validation Set\")\n",
    "        val_rows = get_trackrows(sc, input_file.replace('split', '').format('val'))\n",
    "        val_id = trajectory_type(val_rows, input_file.replace('split', '').format('val'),\n",
    "                                 fps=args.fps, track_id=train_id, args=args)\n",
    "\n",
    "    if test_fraction:\n",
    "        print(\"Categorizing Test Set\")\n",
    "        test_rows = get_trackrows(sc, input_file.replace('split', '').format('test_private'))\n",
    "        _ = trajectory_type(test_rows, input_file.replace('split', '').format('test_private'),\n",
    "                            fps=args.fps, track_id=val_id, args=args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f95fc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--obs_len', type=int, default=25,\n",
    "                        help='Length of observation')\n",
    "    parser.add_argument('--pred_len', type=int, default=25,\n",
    "                        help='Length of prediction')\n",
    "    parser.add_argument('--train_fraction', default=0.6, type=float,\n",
    "                        help='Training set fraction')\n",
    "    parser.add_argument('--val_fraction', default=0.2, type=float,\n",
    "                        help='Validation set fraction')\n",
    "    parser.add_argument('--fps', default=25, type=float,\n",
    "                        help='fps')\n",
    "    parser.add_argument('--order_frames', action='store_true',\n",
    "                        help='For CFF')\n",
    "    parser.add_argument('--chunk_stride', type=int, default=2,\n",
    "                        help='Sampling Stride')\n",
    "    parser.add_argument('--min_length', default=0.0, type=float,\n",
    "                        help='Min Length of Primary Trajectory')\n",
    "    parser.add_argument('--synthetic', action='store_true',\n",
    "                        help='convert synthetic datasets (if false, convert real)')\n",
    "    parser.add_argument('--direct', action='store_true',\n",
    "                        help='directy convert synthetic datasets using commandline')\n",
    "    parser.add_argument('--all_present', action='store_true',\n",
    "                        help='filter scenes where all pedestrians present at all times')\n",
    "    parser.add_argument('--orca_file', default=None,\n",
    "                        help='Txt file for ORCA trajectories, required in direct mode')\n",
    "    parser.add_argument('--goal_file', default=None,\n",
    "                        help='Pkl file for goals (required for ORCA sensitive scene filtering)')\n",
    "    parser.add_argument('--output_filename', default=None,\n",
    "                        help='name of the output dataset filename in .ndjson format, required in direct mode')\n",
    "    parser.add_argument('--mode', default='default', choices=('default', 'trajnet'),\n",
    "                        help='mode of ORCA scene generation (required for ORCA sensitive scene filtering)')\n",
    "\n",
    "    # For Trajectory categorizing and filtering\n",
    "    categorizers = parser.add_argument_group('categorizers')\n",
    "    categorizers.add_argument('--static_threshold', type=float, default=1.0,\n",
    "                              help='Type I static threshold')\n",
    "    categorizers.add_argument('--linear_threshold', type=float, default=0.5,\n",
    "                              help='Type II linear threshold (0.3 for Synthetic)')\n",
    "    categorizers.add_argument('--inter_dist_thresh', type=float, default=5,\n",
    "                              help='Type IIId distance threshold for cone')\n",
    "    categorizers.add_argument('--inter_pos_range', type=float, default=15,\n",
    "                              help='Type IIId angle threshold for cone (degrees)')\n",
    "    categorizers.add_argument('--grp_dist_thresh', type=float, default=0.8,\n",
    "                              help='Type IIIc distance threshold for group')\n",
    "    categorizers.add_argument('--grp_std_thresh', type=float, default=0.2,\n",
    "                              help='Type IIIc std deviation for group')\n",
    "    categorizers.add_argument('--acceptance', nargs='+', type=float, default=[0.1, 1, 1, 1],\n",
    "                              help='acceptance ratio of different trajectory (I, II, III, IV) types')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    # Set Seed\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    sc = pysparkling.Context()\n",
    "\n",
    "    #########################\n",
    "    # Training Set\n",
    "    #########################\n",
    "    args.train_fraction = 1.0\n",
    "    args.val_fraction = 0.0\n",
    "\n",
    "    # 获取csv文件列表\n",
    "    train_dir = 'data/try1/train'\n",
    "    files_csv = [file for file in os.listdir(train_dir) if file.endswith('_tracks.csv')]\n",
    "\n",
    "    # 对每个csv文件执行操作\n",
    "    for file_csv in files_csv:\n",
    "        file_path = os.path.join(train_dir, file_csv)\n",
    "\n",
    "        car = Car()\n",
    "\n",
    "        output_file = f'output_pre/{{split}}/{file_csv[:-4]}.ndjson'\n",
    "        output_path = output_file.replace('output_pre', 'output').format(split='train')\n",
    "\n",
    "        write(car.read_csv(sc, file_path), output_file, args)\n",
    "        categorize(sc, output_file, args)\n",
    "\n",
    "        car.append_scaler(output_path)\n",
    "        car.delete_temp()\n",
    "\n",
    "    #########################\n",
    "    # Validation Set\n",
    "    #########################\n",
    "    args.train_fraction = 0.0\n",
    "    args.val_fraction = 1.0\n",
    "\n",
    "    # 获取csv文件列表\n",
    "    val_dir = 'data/try1/val'\n",
    "    files_csv = [file for file in os.listdir(val_dir) if file.endswith('_tracks.csv')]\n",
    "\n",
    "    # 对每个csv文件执行操作\n",
    "    for file_csv in files_csv:\n",
    "        file_path = os.path.join(val_dir, file_csv)\n",
    "\n",
    "        car = Car()\n",
    "\n",
    "        output_file = f'output_pre/{{split}}/{file_csv[:-4]}.ndjson'\n",
    "        output_path = output_file.replace('output_pre', 'output').format(split='val')\n",
    "\n",
    "        write(car.read_csv(sc, file_path), output_file, args)\n",
    "        categorize(sc, output_file, args)\n",
    "\n",
    "        car.append_scaler(output_path)\n",
    "        car.delete_temp()\n",
    "\n",
    "    #########################\n",
    "    # Testing Set\n",
    "    #########################\n",
    "    args.train_fraction = 0.0\n",
    "    args.val_fraction = 0.0\n",
    "    args.acceptance = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "    # hard breaking # 静止static， 直行linie， 左转left+ turn， 右转right turn， 左变道left lane,右变道right lane\n",
    "    args.chunk_stride = 1\n",
    "\n",
    "    # 获取csv文件列表\n",
    "    test_dir = 'data/try1/test'\n",
    "    files_csv = [file for file in os.listdir(test_dir) if file.endswith('_tracks.csv')]\n",
    "    # 对每个csv文件执行操作\n",
    "    for file_csv in files_csv:\n",
    "        file_path = os.path.join(test_dir, file_csv)\n",
    "\n",
    "        car = Car()\n",
    "\n",
    "        output_file = f'output_pre/{{split}}/{file_csv[:-4]}.ndjson'\n",
    "        output_path = output_file.replace('output_pre', 'output').format(split='test')\n",
    "\n",
    "        write(car.read_csv(sc, file_path), output_file, args)\n",
    "        categorize(sc, output_file, args)\n",
    "\n",
    "        car.append_scaler(output_path)\n",
    "        output_path = output_file.replace('output_pre', 'output').format(split='test_private')\n",
    "        car.append_scaler(output_path)\n",
    "        car.delete_temp()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8697d952",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--obs_len OBS_LEN] [--pred_len PRED_LEN] [--train_fraction TRAIN_FRACTION]\n",
      "                             [--val_fraction VAL_FRACTION] [--fps FPS] [--order_frames] [--chunk_stride CHUNK_STRIDE]\n",
      "                             [--min_length MIN_LENGTH] [--synthetic] [--direct] [--all_present]\n",
      "                             [--orca_file ORCA_FILE] [--goal_file GOAL_FILE] [--output_filename OUTPUT_FILENAME]\n",
      "                             [--mode {default,trajnet}] [--static_threshold STATIC_THRESHOLD]\n",
      "                             [--linear_threshold LINEAR_THRESHOLD] [--inter_dist_thresh INTER_DIST_THRESH]\n",
      "                             [--inter_pos_range INTER_POS_RANGE] [--grp_dist_thresh GRP_DIST_THRESH]\n",
      "                             [--grp_std_thresh GRP_STD_THRESH] [--acceptance ACCEPTANCE [ACCEPTANCE ...]]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\Administrator\\AppData\\Roaming\\jupyter\\runtime\\kernel-2a4da257-68da-4e56-9c8b-c017406a47e7.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\envs\\pytorch\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3465: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d25f7b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
