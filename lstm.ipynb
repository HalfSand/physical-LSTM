{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61327ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import socket\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "from sys import exit\n",
    "from collections import defaultdict, namedtuple\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad1c2fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(path, subset='/train/', sample=1.0, goals=True):\n",
    "    \"\"\" Prepares the train/val scenes and corresponding goals \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    subset: String ['/train/', '/val/']\n",
    "        Determines the subset of data to be processed\n",
    "    sample: Float (0.0, 1.0]\n",
    "        Determines the ratio of data to be sampled\n",
    "    goals: Bool\n",
    "        If true, the goals of each track are extracted\n",
    "        The corresponding goal file must be present in the 'goal_files' folder\n",
    "        The name of the goal file must be the same as the name of the training file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    all_scenes: List\n",
    "        List of all processed scenes\n",
    "    all_goals: Dictionary\n",
    "        Dictionary of goals corresponding to each dataset file.\n",
    "        None if 'goals' argument is False.\n",
    "    Flag: Bool\n",
    "        True if the corresponding folder exists else False.\n",
    "    \"\"\"\n",
    "\n",
    "    ## Check if folder exists\n",
    "    if not os.path.isdir(path + subset):\n",
    "        if 'train' in subset:\n",
    "            print(\"Train folder does NOT exist\")\n",
    "            exit()\n",
    "        if 'val' in subset:\n",
    "            print(\"Validation folder does NOT exist\")\n",
    "            return None, None, False\n",
    "\n",
    "    ## read goal files\n",
    "    all_goals = {}\n",
    "    all_scenes = []\n",
    "\n",
    "    ## List file names\n",
    "    # files = [f.split('.')[-2] for f in os.listdir(path + subset) if f.endswith('.ndjson')]\n",
    "    files = [os.path.splitext(os.path.basename(f))[0] for f in os.listdir(path + subset) if f.endswith('.ndjson')]\n",
    "    #此处应该考虑每个文件的scaler不同。需要把scaler一起读出来。\n",
    "    ## Iterate over file names\n",
    "    for file in files:\n",
    "        reader = Reader(path + subset + file + '.ndjson', scene_type='paths')\n",
    "        ## Necessary modification of train scene to add filename\n",
    "        scene = [(file, s_id, s) for s_id, s in reader.scenes(sample=sample)]\n",
    "        if goals:\n",
    "            goal_dict = pickle.load(open('goal_files/' + subset + file +'.pkl', \"rb\"))\n",
    "            ## Get goals corresponding to train scene\n",
    "            all_goals[file] = {s_id: [goal_dict[path[0].pedestrian] for path in s] for _, s_id, s in scene}\n",
    "        all_scenes += scene\n",
    "\n",
    "    if goals:\n",
    "        return all_scenes, all_goals, True\n",
    "    return all_scenes, None, True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d55780f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(epochs=100):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--epochs', default=epochs, type=int,\n",
    "                        help='number of epochs')\n",
    "    parser.add_argument('--save_every', default=5, type=int,\n",
    "                        help='frequency of saving model (in terms of epochs)')\n",
    "    parser.add_argument('--obs_length', default=25, type=int,\n",
    "                        help='observation length')\n",
    "    parser.add_argument('--pred_length', default=25, type=int,\n",
    "                        help='prediction length')\n",
    "    parser.add_argument('--start_length', default=0, type=int,\n",
    "                        help='starting time step of encoding observation')\n",
    "    parser.add_argument('--batch_size', default=8, type=int)\n",
    "    parser.add_argument('--lr', default=1e-3, type=float,\n",
    "                        help='initial learning rate')\n",
    "    parser.add_argument('--step_size', default=10, type=int,\n",
    "                        help='step_size of lr scheduler')\n",
    "    parser.add_argument('-o', '--output', default=None,\n",
    "                        help='output file')\n",
    "    parser.add_argument('--disable-cuda', action='store_true',\n",
    "                        help='disable CUDA')\n",
    "    parser.add_argument('--path', default='trajdata',\n",
    "                        help='glob expression for data files')\n",
    "    parser.add_argument('--goals', action='store_true',\n",
    "                        help='flag to consider goals of pedestrians')\n",
    "    parser.add_argument('--loss', default='pred', choices=('L2', 'pred'),\n",
    "                        help='loss objective, L2 loss (L2) and Gaussian loss (pred)')\n",
    "    parser.add_argument('--type', default='vanilla',\n",
    "                        choices=('vanilla', 'occupancy', 'directional', 'social', 'hiddenstatemlp',\n",
    "                                 'nn', 'attentionmlp', 'nn_lstm', 'traj_pool'),\n",
    "                        help='type of interaction encoder')\n",
    "    parser.add_argument('--sample', default=1.0, type=float,\n",
    "                        help='sample ratio when loading train/val scenes')\n",
    "    parser.add_argument('--seed', type=int, default=42)\n",
    "\n",
    "    # Augmentations\n",
    "    parser.add_argument('--augment', action='store_true',\n",
    "                        help='perform rotation augmentation')\n",
    "    parser.add_argument('--normalize_scene', action='store_true',\n",
    "                        help='rotate scene so primary pedestrian moves northwards at end of observation')\n",
    "    parser.add_argument('--augment_noise', action='store_true',\n",
    "                        help='flag to add noise to observations for robustness')\n",
    "    parser.add_argument('--obs_dropout', action='store_true',\n",
    "                        help='perform observation length dropout')\n",
    "\n",
    "    # Loading pre-trained models\n",
    "    pretrain = parser.add_argument_group('pretraining')\n",
    "    pretrain.add_argument('--load-state', default=None,\n",
    "                          help='load a pickled model state dictionary before training')\n",
    "    pretrain.add_argument('--load-full-state', default=None,\n",
    "                          help='load a pickled full state dictionary before training')\n",
    "    pretrain.add_argument('--nonstrict-load-state', default=None,\n",
    "                          help='load a pickled state dictionary before training')\n",
    "\n",
    "    # Sequence Encoder Hyperparameters\n",
    "    hyperparameters = parser.add_argument_group('hyperparameters')\n",
    "    hyperparameters.add_argument('--hidden-dim', type=int, default=128,\n",
    "                                 help='LSTM hidden dimension')\n",
    "    hyperparameters.add_argument('--coordinate-embedding-dim', type=int, default=64,\n",
    "                                 help='coordinate embedding dimension')\n",
    "    hyperparameters.add_argument('--pool_dim', type=int, default=256,\n",
    "                                 help='output dimension of interaction vector')\n",
    "    hyperparameters.add_argument('--goal_dim', type=int, default=64,\n",
    "                                 help='goal embedding dimension')\n",
    "\n",
    "    # Grid-based pooling\n",
    "    hyperparameters.add_argument('--cell_side', type=float, default=0.6,\n",
    "                                 help='cell size of real world (in m) for grid-based pooling')\n",
    "    hyperparameters.add_argument('--n', type=int, default=12,\n",
    "                                 help='number of cells per side for grid-based pooling')\n",
    "    hyperparameters.add_argument('--layer_dims', type=int, nargs='*', default=[512],\n",
    "                                 help='interaction module layer dims for gridbased pooling')\n",
    "    hyperparameters.add_argument('--embedding_arch', default='one_layer',\n",
    "                                 help='interaction encoding arch for gridbased pooling')\n",
    "    hyperparameters.add_argument('--pool_constant', default=0, type=int,\n",
    "                                 help='background value (when cell empty) of gridbased pooling')\n",
    "    hyperparameters.add_argument('--norm_pool', action='store_true',\n",
    "                                 help='normalize the scene along direction of movement during grid-based pooling')\n",
    "    hyperparameters.add_argument('--front', action='store_true',\n",
    "                                 help='flag to only consider pedestrian in front during grid-based pooling')\n",
    "    hyperparameters.add_argument('--latent_dim', type=int, default=16,\n",
    "                                 help='latent dimension of encoding hidden dimension during social pooling')\n",
    "    hyperparameters.add_argument('--norm', default=0, type=int,\n",
    "                                 help='normalization scheme for input batch during grid-based pooling')\n",
    "\n",
    "    # Non-Grid-based pooling\n",
    "    hyperparameters.add_argument('--no_vel', action='store_true',\n",
    "                                 help='flag to not consider relative velocity of neighbours')\n",
    "    hyperparameters.add_argument('--spatial_dim', type=int, default=32,\n",
    "                                 help='embedding dimension for relative position')\n",
    "    hyperparameters.add_argument('--vel_dim', type=int, default=32,\n",
    "                                 help='embedding dimension for relative velocity')\n",
    "    hyperparameters.add_argument('--neigh', default=4, type=int,\n",
    "                                 help='number of nearest neighbours to consider')\n",
    "    hyperparameters.add_argument('--mp_iters', default=5, type=int,\n",
    "                                 help='message passing iterations in NMMP')\n",
    "\n",
    "    # Collision Loss\n",
    "    hyperparameters.add_argument('--col_wt', default=0., type=float,\n",
    "                                 help='collision loss weight')\n",
    "    hyperparameters.add_argument('--col_distance', default=0.2, type=float,\n",
    "                                 help='distance threshold post which collision occurs')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Set seed for reproducibility\n",
    "    torch.manual_seed(args.seed)\n",
    "    random.seed(args.seed)\n",
    "\n",
    "    # Define location to save trained model\n",
    "    if not os.path.exists('OUTPUT_BLOCK/{}'.format(args.path)):\n",
    "        os.makedirs('OUTPUT_BLOCK/{}'.format(args.path))\n",
    "    if args.goals:\n",
    "        args.output = 'OUTPUT_BLOCK/{}/lstm_goals_{}_{}.pkl'.format(args.path, args.type, args.output)\n",
    "    else:\n",
    "        args.output = 'OUTPUT_BLOCK/{}/lstm_{}_{}.pkl'.format(args.path, args.type, args.output)\n",
    "\n",
    "    # configure logging\n",
    "    if args.load_full_state:\n",
    "        file_handler = logging.FileHandler(args.output + '.log', mode='a')\n",
    "    else:\n",
    "        file_handler = logging.FileHandler(args.output + '.log', mode='w')\n",
    "    file_handler.setFormatter(jsonlogger.JsonFormatter('%(message)s %(levelname)s %(name)s %(asctime)s'))\n",
    "    stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "    logging.basicConfig(level=logging.INFO, handlers=[stdout_handler, file_handler])\n",
    "    logging.info({\n",
    "        'type': 'process',\n",
    "        'argv': sys.argv,\n",
    "        'args': vars(args),\n",
    "        'version': version,\n",
    "        'hostname': socket.gethostname(),\n",
    "    })\n",
    "\n",
    "    # refactor args for --load-state\n",
    "    # loading a previously saved model\n",
    "    args.load_state_strict = True\n",
    "    if args.nonstrict_load_state:\n",
    "        args.load_state = args.nonstrict_load_state\n",
    "        args.load_state_strict = False\n",
    "    if args.load_full_state:\n",
    "        args.load_state = args.load_full_state\n",
    "\n",
    "    # add args.device\n",
    "    args.device = torch.device('cpu')\n",
    "    if not args.disable_cuda and torch.cuda.is_available():\n",
    "        args.device = torch.device('cuda')\n",
    "\n",
    "    args.path = 'DATA_BLOCK/' + args.path\n",
    "    # Prepare data\n",
    "    train_scenes, train_goals, _ = prepare_data(args.path, subset='/train/', sample=args.sample, goals=args.goals)\n",
    "    val_scenes, val_goals, val_flag = prepare_data(args.path, subset='/val/', sample=args.sample, goals=args.goals)\n",
    "\n",
    "    # pretrained pool model (if any)\n",
    "    pretrained_pool = None\n",
    "\n",
    "    # # create interaction/pooling modules\n",
    "    # pool = None\n",
    "    # if args.type == 'hiddenstatemlp':\n",
    "    #     pool = HiddenStateMLPPooling(hidden_dim=args.hidden_dim, out_dim=args.pool_dim,\n",
    "    #                                  mlp_dim_vel=args.vel_dim)\n",
    "    # elif args.type == 'attentionmlp':\n",
    "    #     pool = AttentionMLPPooling(hidden_dim=args.hidden_dim, out_dim=args.pool_dim,\n",
    "    #                                mlp_dim_spatial=args.spatial_dim, mlp_dim_vel=args.vel_dim)\n",
    "    # elif args.type == 'nn':\n",
    "    #     pool = NearestNeighborMLP(n=args.neigh, out_dim=args.pool_dim, no_vel=args.no_vel)\n",
    "    # elif args.type == 'nn_lstm':\n",
    "    #     pool = NearestNeighborLSTM(n=args.neigh, hidden_dim=args.hidden_dim, out_dim=args.pool_dim)\n",
    "    # elif args.type == 'traj_pool':\n",
    "    #     pool = TrajectronPooling(hidden_dim=args.hidden_dim, out_dim=args.pool_dim)\n",
    "    # elif args.type != 'vanilla':\n",
    "    #     pool = GridBasedPooling(type_=args.type, hidden_dim=args.hidden_dim,\n",
    "    #                             cell_side=args.cell_side, n=args.n, front=args.front,\n",
    "    #                             out_dim=args.pool_dim, embedding_arch=args.embedding_arch,\n",
    "    #                             constant=args.pool_constant, pretrained_pool_encoder=pretrained_pool,\n",
    "    #                             norm=args.norm, layer_dims=args.layer_dims, latent_dim=args.latent_dim)\n",
    "    # hyper parameters\n",
    "    input_size = 8  # frame, car_id, x, y, xVelocity, yVelocity, dx, dy, dvx, dvy\n",
    "    hidden_size = 8  # LSTM hidden size\n",
    "    num_layers = 1  # LSTM layers\n",
    "    output_size = 2  # ax, ay as output\n",
    "\n",
    "    # create forecasting model\n",
    "    model = MyModel(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, output_size=output_size,\n",
    "                    device= args.device)\n",
    "\n",
    "    # optimizer and schedular\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-3)\n",
    "    lr_scheduler = None\n",
    "    if args.step_size is not None:\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.step_size)\n",
    "    start_epoch = 0\n",
    "\n",
    "    # # Loss Criterion\n",
    "    # criterion = L2Loss(col_wt=args.col_wt, col_distance=args.col_distance) if args.loss == 'L2' \\\n",
    "    #                 else PredictionLoss(col_wt=args.col_wt, col_distance=args.col_distance)\n",
    "    #\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    # train\n",
    "    if args.load_state:\n",
    "        # load pretrained model.\n",
    "        # useful for transfer learning\n",
    "        print(\"Loading Model Dict\")\n",
    "        with open(args.load_state, 'rb') as f:\n",
    "            checkpoint = torch.load(f)\n",
    "        pretrained_state_dict = checkpoint['state_dict']\n",
    "        model.load_state_dict(pretrained_state_dict, strict=args.load_state_strict)\n",
    "\n",
    "        if args.load_full_state:\n",
    "            # load optimizers from last training\n",
    "            # useful to continue model training\n",
    "            print(\"Loading Optimizer Dict\")\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            lr_scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "            start_epoch = checkpoint['epoch']\n",
    "\n",
    "    # trainer\n",
    "    trainer = Trainer(model, optimizer=optimizer, lr_scheduler=lr_scheduler, device=args.device,\n",
    "                      criterion=criterion, batch_size=args.batch_size, obs_length=args.obs_length,\n",
    "                      pred_length=args.pred_length, augment=args.augment, normalize_scene=args.normalize_scene,\n",
    "                      save_every=args.save_every, start_length=args.start_length, obs_dropout=args.obs_dropout,\n",
    "                      augment_noise=args.augment_noise, val_flag=val_flag)\n",
    "    trainer.loop(train_scenes, val_scenes, train_goals, val_goals, args.output, epochs=args.epochs, start_epoch=start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d6f5feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrackRow = namedtuple('Row', ['frame', 'car_id', 'x', 'y', 'xVelocity', 'yVelocity','prediction_number', 'scene_id'])\n",
    "TrackRow.__new__.__defaults__ = (None, None, None, None, None, None, None, None)\n",
    "SceneRow = namedtuple('Row', ['scene', 'car_id', 'start', 'end', 'fps', 'tag'])\n",
    "SceneRow.__new__.__defaults__ = (None, None, None, None, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8f411c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def drop_distant(xyv, r=10):\n",
    "    \"\"\"\n",
    "    Drops pedestrians more than r meters away from primary ped\n",
    "    \"\"\"\n",
    "    distance_2 = np.sum(np.square(xyv[:, :, :2] - xyv[:, 0:1, :2]), axis=2)\n",
    "    mask = np.nanmin(distance_2, axis=0) < r ** 2\n",
    "    return xyv[:, mask], mask\n",
    "\n",
    "\n",
    "def rotate_path(path, theta):\n",
    "    ct = math.cos(theta)\n",
    "    st = math.sin(theta)\n",
    "\n",
    "    return [TrackRow(r.frame, r.car_id, ct * r.x + st * r.y, -st * r.x + ct * r.y,\n",
    "                     ct*r.xVelocity + st * r.yVelocity, -st * r.xVelocity + ct * r.yVelocity\n",
    "                     ) for r in path]\n",
    "\n",
    "\n",
    "def random_rotation_of_paths(paths):\n",
    "    theta = random.random() * 2.0 * math.pi\n",
    "    return [rotate_path(path, theta) for path in paths]\n",
    "\n",
    "\n",
    "def random_rotation(xyv, goals=None):\n",
    "    theta = random.random() * 2.0 * math.pi\n",
    "    ct = math.cos(theta)\n",
    "    st = math.sin(theta)\n",
    "    r = np.array([[ct, st], [-st, ct]])\n",
    "\n",
    "    # 旋转位置\n",
    "    rotated_positions = np.einsum('ptc,ci->pti', xyv[:, :, :2], r)\n",
    "\n",
    "    # 旋转速度\n",
    "    rotated_velocities = np.einsum('ptc,ci->pti', xyv[:, :, 2:], r)\n",
    "\n",
    "    # 合并旋转后的位置和速度\n",
    "    rotated_xyv = np.concatenate((rotated_positions, rotated_velocities), axis=2)\n",
    "\n",
    "    if goals is None:\n",
    "        return rotated_xyv\n",
    "    else:\n",
    "        # 如果提供了goals，则旋转goals（假设它们也是[x, y, xVelocity, yVelocity]格式的）\n",
    "        rotated_goals_pos = np.einsum('tc,ci->ti', goals[:, :2], r)\n",
    "        rotated_goals_vel = np.einsum('tc,ci->ti', goals[:, 2:], r)\n",
    "        rotated_goals = np.concatenate((rotated_goals_pos, rotated_goals_vel), axis=1)\n",
    "        return rotated_xyv, rotated_goals\n",
    "\n",
    "\n",
    "def theta_rotation(xyv, theta):\n",
    "    ct = math.cos(theta)\n",
    "    st = math.sin(theta)\n",
    "    r = np.array([[ct, st], [-st, ct]])\n",
    "\n",
    "    # 旋转位置\n",
    "    rotated_positions = np.einsum('ptc,ci->pti', xyv[:, :, :2], r)\n",
    "\n",
    "    # 旋转速度\n",
    "    rotated_velocities = np.einsum('ptc,ci->pti', xyv[:, :, 2:], r)\n",
    "\n",
    "    # 合并旋转后的位置和速度\n",
    "    rotated_xyv = np.concatenate((rotated_positions, rotated_velocities), axis=2)\n",
    "\n",
    "    return rotated_xyv\n",
    "\n",
    "\n",
    "def shift(xyv, center):\n",
    "    # theta = random.random() * 2.0 * math.pi\n",
    "    xyv = xyv - center[np.newaxis, np.newaxis, :]\n",
    "    return xyv\n",
    "\n",
    "\n",
    "def center_scene(xyv, obs_length=9, car_id=0, goals=None):\n",
    "    if goals is not None:\n",
    "        goals = goals[np.newaxis, :, :]\n",
    "    # Center\n",
    "    center = xyv[obs_length-1, car_id]  # Last Observation\n",
    "    xyv = shift(xyv, center)\n",
    "    if goals is not None:\n",
    "        goals = shift(goals, center)\n",
    "    # Rotate\n",
    "    last_obs = xyv[obs_length-1, car_id]\n",
    "    second_last_obs = xyv[obs_length-2, car_id]\n",
    "    diff = np.array([last_obs[0] - second_last_obs[0], last_obs[1] - second_last_obs[1],\n",
    "                        last_obs[2] - second_last_obs[2], last_obs[3] - second_last_obs[3]])\n",
    "    theta = np.arctan2(diff[1], diff[0])\n",
    "    rotation = -theta + np.pi/2\n",
    "    xyv = theta_rotation(xyv, rotation)\n",
    "    if goals is not None:\n",
    "        goals = theta_rotation(goals, rotation)\n",
    "        return xyv, rotation, center, goals[0]\n",
    "    return xyv, rotation, center\n",
    "\n",
    "\n",
    "def inverse_scene(xyv, rotation, center):\n",
    "    xyv = theta_rotation(xyv, -rotation)\n",
    "    xyv = shift(xyv, -center)\n",
    "    return xyv\n",
    "\n",
    "\n",
    "def drop_unobserved(xyv, obs_length=25):\n",
    "    loc_at_obs = xyv[obs_length-1]\n",
    "    absent_at_obs = np.isnan(loc_at_obs).any(axis=1)\n",
    "    mask = ~absent_at_obs\n",
    "    return xyv[:, mask], mask\n",
    "\n",
    "\n",
    "def neigh_nan(xyv):\n",
    "    return np.isnan(xyv).all()\n",
    "\n",
    "\n",
    "def add_noise(observation, thresh=0.005, obs_length=25, ped='primary'):\n",
    "    if ped == 'primary':\n",
    "        observation[:obs_length, 0] += np.random.uniform(-thresh, thresh, observation[:obs_length, 0].shape)\n",
    "    elif ped == 'neigh':\n",
    "        observation[:obs_length, 1:] += np.random.uniform(-thresh, thresh, observation[:obs_length, 1:].shape)\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    return observation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6e029fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Reader(object):\n",
    "    \"\"\"Read trajnet files.\n",
    "\n",
    "    :param scene_type: None -> numpy.array, 'rows' -> TrackRow and SceneRow, 'paths': grouped rows (primary pedestrian first), 'tags': numpy.array and scene tag\n",
    "    :param image_file: Associated image file of the scene\n",
    "    \"\"\"\n",
    "    def __init__(self, input_file, scene_type=None, image_file=None):\n",
    "        if scene_type is not None and scene_type not in {'rows', 'paths', 'tags'}:\n",
    "            raise Exception('scene_type not supported')\n",
    "        self.scene_type = scene_type\n",
    "\n",
    "        self.tracks_by_frame = defaultdict(list)\n",
    "        self.scenes_by_id = dict()\n",
    "        self.scaler = None\n",
    "        self.read_file(input_file)\n",
    "\n",
    "\n",
    "    def read_file(self, input_file):\n",
    "        with open(input_file, 'r') as f:\n",
    "            for line in f:\n",
    "                line = json.loads(line)\n",
    "                track = line.get('track')\n",
    "                if track is not None:\n",
    "                    row = TrackRow(track['f'], track['c'], track['x'], track['y'], track['xVelocity'], track['yVelocity'],\n",
    "                                   track.get('prediction_number'), track.get('scene_id'))\n",
    "                    self.tracks_by_frame[row.frame].append(row)\n",
    "                    continue\n",
    "\n",
    "                scene = line.get('scene')\n",
    "                if scene is not None:\n",
    "                    row = SceneRow(scene['id'], scene['c'], scene['s'], scene['e'],\n",
    "                                   scene.get('fps'), scene.get('tag'))\n",
    "                    self.scenes_by_id[row.scene] = row\n",
    "\n",
    "    def scenes(self, randomize=False, limit=0, ids=None, sample=None):\n",
    "        scene_ids = self.scenes_by_id.keys()\n",
    "        if ids is not None:\n",
    "            scene_ids = ids\n",
    "        if randomize:\n",
    "            scene_ids = list(scene_ids)\n",
    "            random.shuffle(scene_ids)\n",
    "        if limit:\n",
    "            scene_ids = itertools.islice(scene_ids, limit)\n",
    "        if sample is not None:\n",
    "            scene_ids = random.sample(scene_ids, int(len(scene_ids) * sample))\n",
    "        for scene_id in scene_ids:\n",
    "            yield self.scene(scene_id)\n",
    "\n",
    "    @staticmethod\n",
    "    def track_rows_to_paths(primary_car, track_rows):\n",
    "        primary_path = []\n",
    "        other_paths = defaultdict(list)\n",
    "        for row in track_rows:\n",
    "            if row.car_id == primary_car:\n",
    "                primary_path.append(row)\n",
    "                continue\n",
    "            other_paths[row.car_id].append(row)\n",
    "\n",
    "        return [primary_path] + list(other_paths.values())\n",
    "\n",
    "    @staticmethod\n",
    "    def paths_to_xyv(paths):\n",
    "        \"\"\"Convert paths to numpy array with nan as blanks.\"\"\"\n",
    "        frames = set(r.frame for r in paths[0])\n",
    "        cars = set(row.car_id\n",
    "                          for path in paths\n",
    "                          for row in path if row.frame in frames)\n",
    "        paths = [path for path in paths if path[0].car_id in cars]\n",
    "        frames = sorted(frames)\n",
    "        cars = list(cars)\n",
    "\n",
    "        frame_to_index = {frame: i for i, frame in enumerate(frames)}\n",
    "        xyv = np.full((len(frames), len(cars), 4), np.nan)\n",
    "\n",
    "        for car_index, path in enumerate(paths):\n",
    "            for row in path:\n",
    "                if row.frame not in frame_to_index:\n",
    "                    continue\n",
    "                entry = xyv[frame_to_index[row.frame]][car_index]\n",
    "                entry[0] = row.x\n",
    "                entry[1] = row.y\n",
    "                entry[2] = row.xVelocity\n",
    "                entry[3] = row.yVelocity\n",
    "\n",
    "        return xyv\n",
    "\n",
    "    def scene(self, scene_id):\n",
    "        scene = self.scenes_by_id.get(scene_id)\n",
    "        if scene is None:\n",
    "            raise Exception('scene with that id not found')\n",
    "\n",
    "        frames = range(scene.start, scene.end + 1)\n",
    "        track_rows = [r\n",
    "                      for frame in frames\n",
    "                      for r in self.tracks_by_frame.get(frame, [])]\n",
    "\n",
    "        # return as rows\n",
    "        if self.scene_type == 'rows':\n",
    "            return scene_id, scene.id, track_rows\n",
    "\n",
    "        # return as paths\n",
    "        paths = self.track_rows_to_paths(scene.car_id, track_rows)\n",
    "        if self.scene_type == 'paths':\n",
    "            return scene_id, paths\n",
    "\n",
    "        # return with scene tag\n",
    "        if self.scene_type == 'tags':\n",
    "            return scene_id, scene.tag, self.paths_to_xyv(paths)\n",
    "\n",
    "        # return a numpy array\n",
    "        return scene_id, self.paths_to_xyv(paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb748de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, device, scale_factors=None):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, output_size)  # Linear layer\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.l2 = nn.Linear(output_size, output_size)\n",
    "        self.lstm = nn.LSTM(output_size, hidden_size, num_layers)  # LSTM layer\n",
    "        self.reset_parameters()\n",
    "        self.scale_factors = scale_factors \\\n",
    "            if scale_factors is not None else \\\n",
    "                torch.tensor([2, 0.5, 0.01, 0.02]).to(device) # scaler for ind\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "#                 torch.tensor([2, 0.5, 0.01, 0.02]).to(device) # scaler for ind\n",
    "#                 torch.tensor([4.0, 0.3, 0.5, 0.015]).to(device) #scaler for highd\n",
    "    def reset_parameters(self):\n",
    "        # Reset parameters for linear and LSTM layers\n",
    "        self.l1.reset_parameters()\n",
    "        self.l2.reset_parameters()\n",
    "        self.lstm.reset_parameters()\n",
    "\n",
    "    def encoder(self, input_seq, lengths):\n",
    "        fx_fy = self.l1(input_seq)\n",
    "        fx_fy = self.tanh(fx_fy)\n",
    "        fx_fy = self.l2(fx_fy)\n",
    "\n",
    "        # Sort the sequences by length in descending order\n",
    "        lengths, indices = lengths.sort(descending=True)\n",
    "        fx_fy = fx_fy[:, indices]\n",
    "\n",
    "        # Pack the sequences\n",
    "        fx_fy_packed = pack_padded_sequence(fx_fy, lengths.cpu(), enforce_sorted=False)\n",
    "\n",
    "        # Get the outputs from the LSTM\n",
    "        # _, (acc, _) = self.lstm(fx_fy_packed)\n",
    "        output, _ = self.lstm(fx_fy_packed)\n",
    "        output_padded, _ = pad_packed_sequence(output, batch_first=True)\n",
    "        acc = output_padded[:, -1]\n",
    "\n",
    "        # Reshape the output\n",
    "        acc = acc.squeeze(0)\n",
    "\n",
    "        # Restore the original order\n",
    "        _, inv_indices = indices.sort()\n",
    "        acc = acc[inv_indices]\n",
    "\n",
    "        return acc\n",
    "\n",
    "    def decoder(self, last_positions, acc_tensor):\n",
    "        fps = 25\n",
    "        dt = 1 / fps  # Time step\n",
    "\n",
    "        # Extract x, y, x_velocity, y_velocity from the last positions\n",
    "        x, y, x_velocity, y_velocity = torch.chunk(last_positions, 4, dim=-1)\n",
    "\n",
    "        # Compute predicted velocities\n",
    "        x_velocity_pred = x_velocity + dt * acc_tensor[:, 0].unsqueeze(1)\n",
    "        y_velocity_pred = y_velocity + dt * acc_tensor[:, 1].unsqueeze(1)\n",
    "\n",
    "        # Compute predicted positions\n",
    "        x_pred = x + dt * x_velocity_pred\n",
    "        y_pred = y + dt * y_velocity_pred\n",
    "\n",
    "        # Combine the predicted positions and velocities\n",
    "        pred_frame = torch.cat([x_pred, y_pred, x_velocity_pred, y_velocity_pred], dim=-1)\n",
    "\n",
    "        return pred_frame\n",
    "\n",
    "    def forward(self, observed, prediction_truth, batch_split):\n",
    "\n",
    "        # Unscale the observed data\n",
    "        unscaled_observed = observed * self.scale_factors.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        # get all nan lines indices\n",
    "        mask = torch.isnan(unscaled_observed[:, :, 0])\n",
    "        nan_rows = mask.all(dim=0)\n",
    "        indices_to_keep = torch.where(~nan_rows)[0]\n",
    "\n",
    "        # Delete the all nan lines\n",
    "        unscaled_observed = torch.index_select(unscaled_observed, 1, indices_to_keep)\n",
    "        prediction_truth = torch.index_select(prediction_truth, 1, indices_to_keep)\n",
    "\n",
    "        # Update the batch_split\n",
    "        nan_indices = torch.nonzero(nan_rows).squeeze()\n",
    "        for i in range(1, len(batch_split)):\n",
    "            subtract_count = (nan_indices < batch_split[i]).sum().item()\n",
    "            batch_split[i] -= subtract_count\n",
    "\n",
    "        # Mask the observed data\n",
    "        mask = torch.isnan(unscaled_observed[:, :, 0])\n",
    "        unscaled_observed = torch.where(torch.isnan(unscaled_observed), torch.zeros_like(unscaled_observed),\n",
    "                                        unscaled_observed)\n",
    "        lengths = (~mask).sum(dim=0).cpu().long()\n",
    "        pooled_positions = self.generate_pooling(unscaled_observed, batch_split)\n",
    "\n",
    "        # Get each batch's primary_car for truth without loop\n",
    "        primary_indices = batch_split[:-1]\n",
    "        target_truth = prediction_truth[:, primary_indices].to(self.device)\n",
    "        target_pred = torch.zeros_like(target_truth).to(self.device)\n",
    "        pred_frames = target_truth.shape[0]\n",
    "        pred_full = torch.zeros_like(prediction_truth).to(self.device)\n",
    "        acc = torch.zeros_like(target_truth[:, :, :2]).to(self.device)\n",
    "        for i in range(pred_frames):\n",
    "            last_positions = pooled_positions[-1, :, :4]\n",
    "\n",
    "            acc_tensor = self.encoder(pooled_positions, lengths)\n",
    "\n",
    "            # Decode the acc_tensor to get the predicted positions and velocities for the current frame\n",
    "            pred_frame = self.decoder(last_positions, acc_tensor)\n",
    "\n",
    "            # Scale the predictions\n",
    "            scaled_prediction = pred_frame / self.scale_factors.unsqueeze(0)\n",
    "\n",
    "            # Store the prediction 应该是此frame对应的batch数量行，因为是primary cars。列数是4.\n",
    "            target_pred[i] = scaled_prediction[primary_indices]\n",
    "\n",
    "            # Generate pooling for the predicted frame\n",
    "            pooled_pred_frame = self.generate_pooling(pred_frame, batch_split)\n",
    "\n",
    "            # Add the new pooled_pred_frame to the end of pooled_positions and remove the first frame\n",
    "            pooled_positions = torch.cat([pooled_positions[1:], pooled_pred_frame.unsqueeze(0)], dim=0)\n",
    "            \n",
    "            # generate full prediciton to make the visualization possible\n",
    "            pred_full[i] = scaled_prediction\n",
    "            \n",
    "            acc[i] = acc_tensor[primary_indices]\n",
    "        truth_full = prediction_truth\n",
    "            \n",
    "\n",
    "        return target_pred, target_truth, pred_full, truth_full, acc\n",
    "\n",
    "#     def generate_pooling(self, obs_data, batch_split):\n",
    "#         single_frame = False\n",
    "\n",
    "#         # Check if it's a single frame data\n",
    "#         if len(obs_data.shape) == 2:\n",
    "#             single_frame = True\n",
    "#             obs_data = obs_data.unsqueeze(0)  # Add a frame dimension\n",
    "\n",
    "#         num_frames, num_cars, _ = obs_data.shape\n",
    "#         curr_positions = obs_data.clone()  # Clone obs_data to get curr_positions\n",
    "\n",
    "#         # Create a mask where cars with all zeros are considered missing\n",
    "#         valid_cars = (obs_data.sum(dim=2) != 0).unsqueeze(2)\n",
    "\n",
    "#         # Expand dimensions to allow broadcasting\n",
    "#         expanded_obs = obs_data.unsqueeze(2)\n",
    "#         expanded_valid_cars = valid_cars.unsqueeze(2)\n",
    "\n",
    "#         # Calculate the differences using broadcasting\n",
    "#         diffs = expanded_obs - obs_data.unsqueeze(1)\n",
    "\n",
    "#         # Use the valid cars mask to ignore calculations involving missing cars\n",
    "#         diffs *= expanded_valid_cars\n",
    "# #         non_zero = (diffs.all(dim=2) != 0)\n",
    "\n",
    "#         # Sum the differences along the car axis\n",
    "#         rel_positions = diffs.sum(dim=2)\n",
    "\n",
    "#         # Concatenate along the third dimension\n",
    "#         pooled_positions = torch.cat((curr_positions, rel_positions), dim=2)\n",
    "\n",
    "#         # Remove the added frame dimension if it's a single frame data\n",
    "#         if single_frame:\n",
    "#             pooled_positions = pooled_positions.squeeze(0)\n",
    "\n",
    "#         return pooled_positions\n",
    "\n",
    "    def generate_pooling(self, obs_data, batch_split):\n",
    "        single_frame = False\n",
    "\n",
    "        # Check if it's a single frame data\n",
    "        if len(obs_data.shape) == 2:\n",
    "            single_frame = True\n",
    "            obs_data = obs_data.unsqueeze(0)  # Add a frame dimension\n",
    "\n",
    "        num_frames, num_cars, _ = obs_data.shape\n",
    "        curr_positions = obs_data.clone()  # Clone obs_data to get curr_positions\n",
    "\n",
    "        # Create a mask where cars with all zeros are considered missing\n",
    "        valid_cars = (obs_data.sum(dim=2) != 0).unsqueeze(2)\n",
    "\n",
    "        # Expand dimensions to allow broadcasting\n",
    "        expanded_obs = obs_data.unsqueeze(2)\n",
    "        expanded_valid_cars = valid_cars.unsqueeze(2)\n",
    "\n",
    "        # Calculate the differences using broadcasting\n",
    "        diffs = expanded_obs - obs_data.unsqueeze(1)\n",
    "\n",
    "        eps = 1e-10\n",
    "        diffs[diffs == 0] += eps  # Add a very small value to zero entries\n",
    "\n",
    "        # Calculate the inverse\n",
    "        diffs_inv = 1.0 / diffs\n",
    "\n",
    "        # Set values that are extremely large (because of the division by a small value) to zero\n",
    "        threshold = 1e9  # You can adjust this threshold as needed\n",
    "        diffs_inv[diffs_inv.abs() > threshold] = 0\n",
    "\n",
    "        # Use the valid cars mask to ignore calculations involving missing cars\n",
    "        diffs_inv *= expanded_valid_cars\n",
    "\n",
    "        # Sum the inversed differences along the car axis\n",
    "        scores = diffs_inv.sum(dim=2)\n",
    "\n",
    "        # Concatenate along the third dimension\n",
    "        pooled_positions = torch.cat((curr_positions, scores), dim=2)\n",
    "\n",
    "        # Remove the added frame dimension if it's a single frame data\n",
    "        if single_frame:\n",
    "            pooled_positions = pooled_positions.squeeze(0)\n",
    "\n",
    "        return pooled_positions\n",
    "\n",
    "\n",
    "\n",
    "class LSTMPredictor(object):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def save(self, state, filename):\n",
    "        with open(filename, 'wb') as f:\n",
    "            torch.save(self, f)\n",
    "\n",
    "        # # during development, good for compatibility across API changes:\n",
    "        # # Save state for optimizer to continue training in future\n",
    "        with open(filename + '.state', 'wb') as f:\n",
    "            torch.save(state, f)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            return torch.load(f)\n",
    "\n",
    "    def __call__(self, paths, scene_goal, n_predict=12, modes=1, predict_all=True, obs_length=25, start_length=0,\n",
    "                 args=None):\n",
    "        self.model.eval()\n",
    "        # self.model.train()\n",
    "        with torch.no_grad():\n",
    "            xyv = Reader.paths_to_xyv(paths)\n",
    "            # xyv = add_noise(xyv, thresh=args.thresh, ped=args.ped_type)\n",
    "            batch_split = [0, xyv.shape[1]]\n",
    "\n",
    "            if args.normalize_scene:\n",
    "                xyv, rotation, center, scene_goal = center_scene(xyv, obs_length, goals=scene_goal)\n",
    "\n",
    "            xyv = torch.Tensor(xyv)  # .to(self.device)\n",
    "            scene_goal = torch.Tensor(scene_goal)  # .to(device)\n",
    "            batch_split = torch.Tensor(batch_split).long()\n",
    "\n",
    "            multimodal_outputs = {}\n",
    "            for num_p in range(modes):\n",
    "                # _, output_scenes = self.model(xyv[start_length:obs_length], scene_goal, batch_split, xyv[obs_length:-1].clone())\n",
    "                _, output_scenes = self.model(xyv[start_length:obs_length], scene_goal, batch_split, n_predict=n_predict)\n",
    "                output_scenes = output_scenes.numpy()\n",
    "                if args.normalize_scene:\n",
    "                    output_scenes = inverse_scene(output_scenes, rotation, center)\n",
    "                output_primary = output_scenes[-n_predict:, 0]\n",
    "                output_neighs = output_scenes[-n_predict:, 1:]\n",
    "                ## Dictionary of predictions. Each key corresponds to one mode\n",
    "                multimodal_outputs[num_p] = [output_primary, output_neighs]\n",
    "\n",
    "        ## Return Dictionary of predictions. Each key corresponds to one mode\n",
    "        return multimodal_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feddf223",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "# hyper parameters\n",
    "input_size = 8  # frame, car_id, x, y, xVelocity, yVelocity, dx, dy, dvx, dvy\n",
    "hidden_size = 2  # LSTM hidden size\n",
    "num_layers = 2  # LSTM layers\n",
    "output_size = 2  # ax, ay as output\n",
    "\n",
    "# create forecasting model\n",
    "model = MyModel(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, output_size=output_size,\n",
    "                device=device)\n",
    "model.to(device)\n",
    "criterion = nn.MSELoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a8197d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_scenes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10556\\3797628325.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mbatch_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mscene_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscene_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_scenes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;31m## make new scene\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mscene\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mReader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpaths_to_xyv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_scenes' is not defined"
     ]
    }
   ],
   "source": [
    "## Initialize batch of scenes\n",
    "batch_size = 10\n",
    "batch_scene = []\n",
    "batch_split = [0]\n",
    "\n",
    "for scene_i, (filename, scene_id, paths) in enumerate(train_scenes):\n",
    "    ## make new scene\n",
    "    scene = Reader.paths_to_xyv(paths)\n",
    "\n",
    "    #scene: frame_index x car_index x xyv\n",
    "    ## Augment scene to batch of scenes\n",
    "    batch_scene.append(scene)\n",
    "    batch_split.append(int(scene.shape[1]))\n",
    "\n",
    "    if ((scene_i + 1) % batch_size == 0) or ((scene_i + 1) == len(train_scenes)):\n",
    "        ## Construct Batch torch.Size([50, 317, 4])\n",
    "        batch_scene = np.concatenate(batch_scene, axis=1)\n",
    "        batch_split = np.cumsum(batch_split)\n",
    "        batch_scene = torch.Tensor(batch_scene).to(device)\n",
    "        batch_split = torch.Tensor(batch_split).to(device).long()\n",
    "        observed = batch_scene[0:25].clone()\n",
    "        prediction_truth = batch_scene[25:50].clone()\n",
    "        # 25frames x 10primary cars x [x, y, xV, yV]\n",
    "        train_pred, train_truth = model(observed, prediction_truth, batch_split)\n",
    "        print('train_pred:\\n',train_pred,train_pred.shape)\n",
    "        print('train_truth:\\n',train_truth,train_truth.shape)\n",
    "        loss = criterion(train_pred, train_truth)\n",
    "        print('loss:\\n', loss)\n",
    "        break\n",
    "        ## Reset Batch\n",
    "        batch_scene = []\n",
    "        batch_scene_goal = []\n",
    "        batch_split = [0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b1573f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Prepare data\n",
    "test_scenes, test_goals, _ = prepare_data(path='DATA_BLOCK/ind' , subset='/test/', sample=1, goals=0)\n",
    "#for train_scenes: file, scene_id, scene. scene = 50 * car_ids * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5c164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, optimizer=None, path='default_path', model_type='vanilla', output='default_output', return_epoch=False, return_losses=False):\n",
    "    # Construct the paths based on the manually provided parameters\n",
    "    model_file_path = 'OUTPUT_BLOCK/{}/lstm_{}_{}.pkl'.format(path, model_type, output)\n",
    "    state_file_path = 'OUTPUT_BLOCK/{}/lstm_{}_{}.state'.format(path, model_type, output)\n",
    "    \n",
    "    # Load the model architecture and weights\n",
    "    loaded_model = torch.load(model_file_path)\n",
    "    model.load_state_dict(loaded_model.model.state_dict())\n",
    "\n",
    "    # Load the model's state (e.g. optimizer state, epoch, losses)\n",
    "    checkpoint = torch.load(state_file_path)\n",
    "\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    results = []\n",
    "    if return_epoch:\n",
    "        epoch = checkpoint.get('epoch', None)\n",
    "        results.append(epoch)\n",
    "    if return_losses:\n",
    "        train_losses = checkpoint.get('train_losses', [])\n",
    "        val_losses = checkpoint.get('val_losses', [])\n",
    "        results.extend([train_losses, val_losses])\n",
    "\n",
    "    return results if len(results) > 0 else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e8c1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_scale=torch.tensor([4.0, 0.3, 0.5, 0.015]).to(device)\n",
    "train_scale=torch.tensor([2, 0.5, 0.01, 0.02]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ebd584",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model(model, path='ind1', output='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52e8e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Traj_plot(scene_index, train_scenes, train_scale, model, criterion, device):\n",
    "    model.eval()\n",
    "\n",
    "    # Extract specific scene from train_scenes based on scene_index\n",
    "    filename, scene_id, paths = train_scenes[scene_index]\n",
    "    scene = Reader.paths_to_xyv(paths)\n",
    "    scene_tensor = torch.Tensor(scene).to(device)\n",
    "    observed = scene_tensor[0:25]\n",
    "    prediction_truth = scene_tensor[25:50]\n",
    "\n",
    "    # Create a virtual batch\n",
    "    num_copies = 3\n",
    "    observed_batch = observed.repeat(1, num_copies, 1)\n",
    "    prediction_truth_batch = prediction_truth.repeat(1, num_copies, 1)\n",
    "    \n",
    "    # Compute the original batch_split\n",
    "    original_batch_split = torch.tensor([observed.shape[1]]).to(device)\n",
    "    batch_split = original_batch_split.repeat(num_copies)\n",
    "    cumulative_batch_split = torch.cumsum(batch_split, dim=0)\n",
    "\n",
    "    # Compute the predictions\n",
    "    with torch.no_grad():\n",
    "        train_pred_batch, train_truth_batch, pred_full_batch, truth_full_batch, acc = model(observed_batch, prediction_truth_batch, cumulative_batch_split)\n",
    "\n",
    "    # Use the first result from the virtual batch\n",
    "    train_pred = train_pred_batch[:, 1]\n",
    "    train_truth = train_truth_batch[:, 1]\n",
    "    acc = acc[:, 1].cpu().numpy()\n",
    "\n",
    "    pred_full = pred_full_batch\n",
    "    truth_full = truth_full_batch\n",
    "    \n",
    "    \n",
    "    # Calculate the loss\n",
    "    train_loss = criterion(train_pred.squeeze()[:, :4], train_truth.squeeze()[:, :4])\n",
    "\n",
    "    # Convert to numpy for plotting\n",
    "    observed_np = (observed_batch * train_scale).cpu().numpy()\n",
    "    train_pred_np = (train_pred * train_scale).cpu().numpy()\n",
    "    train_truth_np = (train_truth * train_scale).cpu().numpy()\n",
    "    \n",
    "    pred_full_np = (pred_full * train_scale.unsqueeze(0).unsqueeze(0)).cpu().numpy()\n",
    "    truth_full_np = (truth_full * train_scale.unsqueeze(0).unsqueeze(0)).cpu().numpy()\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(observed_np[:, 0, 0], observed_np[:, 0, 1], 'b--', linewidth=2, label='Traj_observed')\n",
    "    plt.scatter(observed_np[:, 0, 0], observed_np[:, 0, 1], c='b', s=5)\n",
    "    \n",
    "    plt.plot(train_pred_np[:, 0], train_pred_np[:, 1], 'g-', label='Traj_pred')\n",
    "    plt.scatter(train_pred_np[:, 0], train_pred_np[:, 1], c='g', s=5)\n",
    "\n",
    "    plt.plot(train_truth_np[:, 0], train_truth_np[:, 1], 'r-', label='Traj_target')\n",
    "    plt.scatter(train_truth_np[:, 0], train_truth_np[:, 1], c='r', s=5)\n",
    "\n",
    "    plt.scatter(observed_np[-1, 0, 0], observed_np[-1, 0, 1], marker='^', s=100, c='black', label='Current State')\n",
    "    train_loss = train_loss.item()\n",
    "    plt.text(0.95, 0.01, f'Loss: {train_loss:.2f}', verticalalignment='bottom', horizontalalignment='right', transform=plt.gca().transAxes)\n",
    "\n",
    "    \n",
    "#     ### others\n",
    "#     plt.plot(pred_full_np[:, 1:, 0], pred_full_np[:, 1:, 1], 'y-')\n",
    "#     plt.scatter(pred_full_np[:, 1:, 0], pred_full_np[:, 1:, 1], c='y', s=5)\n",
    "\n",
    "#     plt.plot(truth_full_np[:, 1:, 0], truth_full_np[:, 1:, 1], 'r-')\n",
    "#     plt.scatter(truth_full_np[:, 1:, 0], truth_full_np[:, 1:, 1], c='r', s=5)\n",
    "\n",
    "    # 使用 quiver 来添加加速度向量\n",
    "    plt.quiver(train_pred_np[:, 0], \n",
    "               train_pred_np[:, 1], \n",
    "               acc[:, 0], \n",
    "               acc[:, 1], \n",
    "               angles='xy', \n",
    "               scale_units='xy', \n",
    "               scale=1, \n",
    "               color='y',\n",
    "               width=0.003,\n",
    "               headwidth=3, \n",
    "               headlength=4, \n",
    "               headaxislength=3.5,\n",
    "               label='force'\n",
    "              )\n",
    "    \n",
    "    plt.axis('equal') \n",
    "    plt.legend()\n",
    "    plt.savefig('Traj_plot.png')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1780a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Traj_plot(400, test_scenes, train_scale, model, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab8047d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
